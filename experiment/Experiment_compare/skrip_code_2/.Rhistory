K <- length(vec)/p^2
A <- list()
for (i in 1:K){
A[[i]] <- matrix(vec[(1+(i-1)*(p^2)):(i*p^2)],p,p,byrow=TRUE)
}
return(A)
}
#################################################################################
#################################################################################
### IN THAT SECTION I HAVE ALL THE POSSIBLE CRITERIONS                     ######
### FOR PICKING SPARSITY AND FUSION PARAMETERS                             ######
###                                                                        ######
### The parameters for all of them will be:                                ######
###                                                                        ######
###  Est - set of estimates that we calculate the criterion for,           ######
###  X - data matrix,                                                      ######
###  Y - response vector,                                                  ######
###  lambda.path - vector of sparsity parameter values                     ######
###                that estimates from 'Est' correspond to,                ######
###  df.coef - the coefficient for degrees of freedom to be multiplied by  ######
###                                                                        ######
### The functions will return:                                             ######
###                                                                        ######
###  the estimate that minimizes the criterion,                            ######
###  the corresponding minimum value of the criterion,                     ######
###  set of criterion values for all possible sparsity parameter values,   ######
###  sparsity parameter value corresponding to minimum criterion value,    ######
###  index of that sparsity parameter value in lambda.path,                ######
###  log-likelihood part of the criterion(for the whole lambda path)       ######
###  degrees of freedom of the criterion(for the whole lambda path)        ######
#################################################################################
#################################################################################
################################################
### AIC criterion ##############################
###############################################
AIC <- function(Est,X,Y,lambda.path,df.path,p,K=1){
AIC <- rep(0,length(lambda.path))
loglik.part <- rep(0,length(lambda.path))
df.part <- rep(0,length(lambda.path))
n <- nrow(Y)
t <- n/p + 1
for(i in 1:length(lambda.path)){
df <- df.path[i]
loglik.part[i] <- 2*n*log(norm(Y - X %*% Est[,i],type="F")/sqrt(n))
df.part[i] <- df
AIC[i] <- loglik.part[i] + df.part[i]
}
min <- which.min(AIC)
return(list(Est=Est[,min],
Criter.min=AIC[min],
Criter=AIC,
lambda1=lambda.path[min],
ind=min,
loglik.part=loglik.part,
df.part=df.part))
}
################################################
### AICc criterion ##############################
###############################################
AICc <- function(Est,X,Y,p,K=1,lambda.path,df.path){
AIC <- rep(0,length(lambda.path))
loglik.part <- rep(0,length(lambda.path))
df.part <- rep(0,length(lambda.path))
n <- nrow(Y)
t <- n/p + 1
for(i in 1:length(lambda.path)){
df <- df.path[i]
loglik.part[i] <- 2*n*log(norm(Y - X %*% Est[,i],type="F")/sqrt(n))
df.part[i] <- 2*df + 2*df*(df+1)/(n-df-1)
AIC[i] <- loglik.part[i] + df.part[i]
}
min <- which.min(AIC)
return(list(Est=Est[,min],
Criter.min=AIC[min],
Criter=AIC,
lambda1=lambda.path[min],
ind=min,
loglik.part=loglik.part,
df.part=df.part))
}
################################################
### BIC criterion ##############################
###############################################
BIC <- function(Est,X,Y,K,p,df.path,lambda.path){
BIC <- rep(0,ncol(Est))
n <- nrow(Y)
t <- n/K + 1
loglik.part <- rep(0,length(lambda.path))
df.part <- rep(0,length(lambda.path))
l <- length(df.path)
for(i in 1:l){
df <- df.path[i]
loglik.part[i] <- 2*n*log(norm(Y - X %*% Est[,i],type="F")/sqrt(n))
df.part[i] <- df*log(n)
BIC[i] <- loglik.part[i] + df*log(n)
}
min <- which.min(BIC)
return(list(Est=Est[,min],
Criter.min=BIC[min],
Criter=BIC,
lambda1=lambda.path[min],
ind=min,
loglik.part=loglik.part,
df.part=df.part))
}
##########################################
#### Calculating Matthews coefficient based on TP,FP,TN,FN rates
#### of the estimate
##########################################
Matthews.Coef <- function(TP,FP,TN,FN){
return(ifelse(TP*TN - FP*FN == 0,0,(TP*TN - FP*FN)/sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))))
}
##################
## MAKES A LIST OF n ELEMENTS
## Elem - type of the elements(matrices,lists, whatever it may be)
##################
make.list <- function(elem,n){
res.list <- list()
for(i in 1:n){
res.list[[i]] <- elem
}
return(res.list)
}
###########################
### Compact Matrix Form Setup function (as in Problem Setup section of the paper)
### Transforms the K original p-dimensional time series of length n.tp into matrices C and X of the regression problem.
### From section 2.3 of the paper, algorithm description for arbitrary j=1,...p:
###   C denotes X^{tilde}_j from the paper, X denotes B^{tilde} from the paper,
###   C = X*(x_j^C + x_j^I) + errors
###########################
mat.setup <- function( # outputs matrices C (X^{tilde}_j) and X (B^{tilde}) for regression setup in section 2.3
ts,      # time series matrix, rows - variables, columns - time points
n.tp,    # number of observed time points T
K,       # number of subjects
p,       # number of variables per subject
D=1      # order of VAR model
){
C.mat <- matrix(0,n.tp-D,K*p)
for (i in 1:(n.tp-D)){
for (k in 1:K){
C.mat[i,((k-1)*p + 1):(k*p)] <- c(ts[((k-1)*p + 1):(k*p),(i+D)])
}
}
B.mat <- matrix(0,n.tp-D,K*(D*p))
for (i in 1:(n.tp-D)){
for (k in 1:K){
res <- ts[((k-1)*p + 1):(k*p),i]
if (D>1){
for (d in 2:D){
res <- c(ts[((k-1)*p + 1):(k*p),i+(d-1)],res)
}
}
B.mat[i,((k-1)*(D*p) + 1):(k*(D*p))] <- res
}
}
B.list.mat <- list()
for(k in 1:K){
B.list.mat[[k]] <- B.mat[,((k-1)*(D*p) + 1):(k*(D*p))]
}
X.list.mat <- block.diag(B.list.mat)
return(list(C=C.mat,
X=X.list.mat))
}
#########################################################
## Product of matrix M1 with block-diagonal matrix M2  ##
## Matrix M2 consists of identical blocks "Block"      ##
## Optimal calculation procedure through calculating   ##
## products of smaller submatrices                     ##
#########################################################
optim.product <- function(#returns product of matrix M1 by block-diagonal matrix M2
M1,
Block,    # block of M2 matrix
nblocks   # number of diagonal blocks in M2
){
dim1 <- nrow(M1)
Prod <- matrix(0,dim1,ncol(Block)*nblocks)
p <- nblocks
D <- dim(Block)[2]/p
t <- dim1/nblocks
for (i in 1:nblocks)
for (j in 1:nblocks)
Prod[(i-1)*t + 1:t,(j-1)*(D*p) + 1:(D*p)] <- M1[(i-1)*t + 1:t,(j-1)*t + 1:t] %*% Block
return(Prod)
}
########################################
#### OLS estimation for a time series ##
########################################
OLS.tseries <- function(# returns the OLS estimate for transition matrices,
#         maximum likelihood estimate of variance
Data, # time series matrix, rows - variables, columns - time points
D=1   # order of VAR model for the OLS fit
){
p <- nrow(Data)
t <- ncol(Data)
### Setting up all the matrices for regression problem
### Response vector and data matrix
Response <- matrix(c(t(Data[,c(t:(D+1))])))
X <- NULL
for (d in c(1:D)) X <- cbind(X,matrix(Data[,((t-d):(D-d+1))],t-D,p,byrow=TRUE))
X <- optim.product(diag(1,p) %x% diag(1,t-D),X,p)
X.full <- X
Y.full <- Response
beta.hat <- solve(t(X.full)%*%X.full)%*%(t(X.full)%*%Y.full)
sigma2 <- sum((Y.full - X.full%*%beta.hat)^2)/nrow(X.full)
return(list(beta.hat=beta.hat,
sigma2=sigma2))
}
##############################################################################
#### Sparse estimation for a time series                                  ####
#### Used for SPARSE LASSO method from comparative study in Section 3     ####
##############################################################################
Sparse.tseries <- function(# returns the sparse estimate for transition matrices,
Data, # time series matrix, rows - variables, columns - time points
sigma2, # sigma^2 for error covariance matrix
criter="BIC", # criteria to be used for sparsity parameter selection
D=1   # order of VAR model for the OLS fit
){
p <- nrow(Data)
t <- ncol(Data)
### Setting up all the matrices for regression problem
### Response vector and data matrix
Response <- matrix(c(t(Data[,c(t:(D+1))])))
X <- NULL
for (d in c(1:D)) X <- cbind(X,matrix(Data[,((t-d):(D-d+1))],t-D,p,byrow=TRUE))
X <- optim.product(diag(1,p) %x% diag(1,t-D),X,p)
X.full <- X
Y.full <- Response
Sigma <- diag(sigma2, nrow(X.full))
r <- glmnet(solve(Sigma) %*% X.full,
solve(Sigma) %*% Y.full,
family="gaussian",
standardize=standardize,
intercept=intercept)
lambda_SPARS.path <- r$lambda
est <- r$beta
sep.df <- r$df
### Tuning parameter selection
if (criter == "AIC")
sep.est.out <- AIC(as.matrix(est),X.full,as.matrix(Y.full),p=p,K=K,df.path=sep.df,lambda.path=lambda_SPARS.path)
if (criter == "BIC")
sep.est.out <- BIC(as.matrix(est),X.full,as.matrix(Y.full),p=p,K=K,df.path=sep.df,lambda.path=lambda_SPARS.path)
if (criter == "AICc")
sep.est.out <- AICc(as.matrix(est),X.full,as.matrix(Y.full),p=p,K=K,df.path=sep.df,lambda.path=lambda_SPARS.path)
return(list(beta.hat=sep.est.out$Est))
}
# put data in format same as an input for mat.setup
# variable D, p, K
D <-1
p <- 20
t <- 100
K <- 50
density = c(1,5)
realz <- 4
for (dd in 2:2){
for (itr in 2:realz){
namedir <- './data_R_formulationD/'
tmp <- read.csv(file = paste(namedir,'K',K,'_data_',density[dd],'percent_',itr,'.csv',sep=""),header=FALSE)
DATA <-as.matrix(tmp)
colnames(DATA) <- NULL
sigma2 <- rep(0,K)
sds <- apply(DATA,1,function(x) sd(x))
for (k in 1:K) sigma2[k] <- OLS.tseries(DATA[(k-1)*p + 1:p,],D=D)$sigma2
Group.Est <- make.list(matrix(0,D*p,p),K)
Sep.Est.Second <-  make.list(matrix(0,D*p,p),K)
Group.Final <- make.list(matrix(0,D*p,p),K)
#A.list <- list()
#
#for (d in 1:D){
#  A.full <- list()
#  for (k in 1:K) A.full[[k]] <- A.true$A.true[[k]][[d]]
#  A.list[[d]] <- block.diag(A.full)
#}
Sigma <- list()
for(i in 1:K) Sigma[[i]] <- diag(1,p)               # simply setting the error covariance to be identity matrix
Sigma.full <- block.diag(Sigma)
#############################
### FULL PROBLEM SETUP   ####
#############################
M.setup <- mat.setup(DATA,t,K,p,D=D)
C.list <- M.setup$C
X.list <- M.setup$X
### Vector of group number assignments for group lasso
group <- c(1:(D*p))
if (K>1){
for (j in 2:K) group <- c(group,1:(D*p))
}
## Initializing vectors to contain estimates during algorithm iterations
grouplasso.result_before <- make.list(numeric(D*K*p),p)
seplasso.result_before <- make.list(numeric(D*K*p),p)
seplasso.result <- make.list(numeric(D*K*p),p)
grouplasso.result <- make.list(numeric(D*K*p),p)
gl.zeros <- list()
## Going column-by-column, j - column index.
for (j in 1:p){
cat("\n")
print(paste("j:",j,sep=""))
# cat("\n")
it <- 0
flag <- 0
### First 5 iterations to tune up the regularization parameters.
### (or less than 5, if regularization parameter values stop changing => we can just fix them)
while ((it<5) & (flag==0)){
it <- it+1
#print(paste("Tune Iter:",it,sep=""))
###############################################################
## FIRST STAGE: Group lasso estimation of common component ####
###############################################################
### Initializing response vector and data matrix for standard regression problems for the first stage
Y <- numeric(K*(t-D))
Xbeta <- X.list %*% seplasso.result[[j]]
for (k in 1:K){
Y[(k-1)*(t-D) + (1:(t-D))] <- C.list[1:(t-D),j + (k-1)*p] - Xbeta[(k-1)*(t-D) + (1:(t-D))]
}
### Doing group lasso optimization
D.sigma <- sqrt(diag(c(sapply(sigma2, function(x) return(rep(x,(t-D)))))))
r <- grpreg(solve(D.sigma) %*% X.list,
solve(D.sigma) %*% Y,
group=group,
penalty="grLasso",
family="gaussian",
intercept=intercept,
warn=FALSE)
# lambda=lambda.group)
lambda_G.path <- r$lambda
est <- r$beta[-1,]
grouplasso.result.df <- r$df
### Tuning parameter selection
if (criter == "AIC")
group.est.out <- AIC(as.matrix(est),X.list,as.matrix(Y),p=p,K=K,lambda.path=lambda_G.path,df.path=grouplasso.result.df)
if (criter == "BIC")
group.est.out <- BIC(as.matrix(est),X.list,as.matrix(Y),p=p,K=K,df.path=grouplasso.result.df,lambda.path=lambda_G.path)
if (criter == "AICc")
group.est.out <- AICc(as.matrix(est),X.list,as.matrix(Y),p=p,K=K,df.path=grouplasso.result.df,lambda.path=lambda_G.path)
# grouplasso.result[[j]] <- sparsify(group.est.out$Est, Thresh2)
grouplasso.result[[j]] <- group.est.out$Est
lambda.group <- group.est.out$lambda1
####################################################################
## SECOND STAGE: Sparse lasso estimation of individual component ###
####################################################################
### Initializing response vector and data matrix for standard regression problem for the second stage
gl.zeros[[j]] <- (grouplasso.result[[j]] == 0)
Xbeta <- X.list %*% grouplasso.result[[j]]
for (k in 1:K){
Y[(k-1)*(t-D) + (1:(t-D))] <- C.list[1:(t-D),j + (k-1)*p] - Xbeta[(k-1)*(t-D) + (1:(t-D))]
}
X.zero <- X.list[,gl.zeros[[j]]]
### Doing sparse lasso optimization
r <- glmnet(solve(D.sigma) %*% X.zero,
solve(D.sigma) %*% Y,
family="gaussian",
standardize=standardize,
intercept=intercept)
lambda_SPARS.path <- r$lambda
est <- r$beta
sep.df <- r$df
### Tuning parameter selection
if (criter.second == "AIC")
sep.est.out <- AIC(as.matrix(est),X.zero,as.matrix(Y),p=p,K=K,df.path=sep.df,lambda.path=lambda_SPARS.path)
if (criter.second == "BIC")
sep.est.out <- BIC(as.matrix(est),X.zero,as.matrix(Y),p=p,K=K,df.path=sep.df,lambda.path=lambda_SPARS.path)
if (criter.second == "AICc")
sep.est.out <- AICc(as.matrix(est),X.zero,as.matrix(Y),p=p,K=K,df.path=sep.df,lambda.path=lambda_SPARS.path)
lambda.sparse <- sep.est.out$lambda1
seplasso.result[[j]] <- rep(0,D*p*K)
# seplasso.result[[j]][gl.zeros[[j]]] <- sparsify(sep.est.out$Est, Thresh2)
seplasso.result[[j]][gl.zeros[[j]]] <- sep.est.out$Est
### Recording estimates
for (k in 1:K){
Group.Est[[k]][j,] <- grouplasso.result[[j]][(k-1)*p+(D-1)*(p) + (1:p)]
Sep.Est.Second[[k]][j,] <- seplasso.result[[j]][(k-1)*p+(D-1)*(p) + (1:p)]
}
if (it>1){
#####
## STOPPING CRITERION for the tuning parameter initialization steps (if it hasn't reached 5 iterations yet)
#####
diff_magnitudes <- sum((c(grouplasso.result[[j]],seplasso.result[[j]])-c(grouplasso.result_before[[j]],seplasso.result_before[[j]]))^2)
## Printing out current difference in optimized values for consecutive iterations.
print(paste("Tune-up:", diff_magnitudes))
if (diff_magnitudes < eps){
flag <- 1
#n.iter[run,j] <- it
}
}
### keeping track of estimates from previous iterations
grouplasso.result_before <- grouplasso.result
seplasso.result_before <- seplasso.result
}
# cat("\n")
# print("lambdas selected")
# print(paste("Group lambda:", lambda.group))
# print(paste("Sparse lambda:", lambda.sparse))
# cat("\n")
#####
## Now, FIX the values of lambda.group and lambda.sparse.
## Iterate until convergence for fixed lambda.group and lambda.sparse
#####
it <- 0
flag <- 0
while ((it<max.iter) & (flag==0)){
it <- it+1
#  print(paste("Iter:",it,sep=""))
###############################################################
## FIRST STAGE: Group lasso estimation of common component ####
###############################################################
### Initializing response vector and data matrix for standard regression problems for the first stage
Y <- numeric(K*(t-D))
Xbeta <- X.list %*% seplasso.result[[j]]
for (k in 1:K){
Y[(k-1)*(t-D) + (1:(t-D))] <- C.list[1:(t-D),j + (k-1)*p] - Xbeta[(k-1)*(t-D) + (1:(t-D))]
}
### Doing group lasso optimization
D.sigma <- sqrt(diag(c(sapply(sigma2, function(x) return(rep(x,(t-D)))))))
r <- grpreg(solve(D.sigma) %*% X.list,
solve(D.sigma) %*% Y,
group=group,
penalty="grLasso",
family="gaussian",
intercept=intercept,
warn=FALSE,
lambda=lambda.group)
#lambda_G.path <- r$lambda
est <- r$beta[-1]
grouplasso.result.df <- r$df
#grouplasso.result[[j]] <- sparsify(est, Thresh2)
grouplasso.result[[j]] <- est
# lambda.group <- group.est.out$lambda1
####################################################################
## SECOND STAGE: Sparse lasso estimation of individual component ###
####################################################################
### Initializing response vector and data matrix for standard regression problem for the second stage
gl.zeros[[j]] <- (grouplasso.result[[j]] == 0)
Xbeta <- X.list %*% grouplasso.result[[j]]
for (k in 1:K){
Y[(k-1)*(t-D) + (1:(t-D))] <- C.list[1:(t-D),j + (k-1)*p] - Xbeta[(k-1)*(t-D) + (1:(t-D))]
}
X.zero <- X.list[,gl.zeros[[j]]]
### Doing sparse lasso optimization
r <- glmnet(solve(D.sigma) %*% X.zero,
solve(D.sigma) %*% Y,
family="gaussian",
standardize=standardize,
intercept=intercept,
lambda=lambda.sparse)
#lambda_SPARS.path <- r$lambda
est <- as.matrix(r$beta)
sep.df <- r$df
#lambda.sparse <- sep.est.out$lambda1
seplasso.result[[j]] <- rep(0,D*p*K)
# seplasso.result[[j]][gl.zeros[[j]]] <- sparsify(sep.est.out$Est, Thresh2)
seplasso.result[[j]][gl.zeros[[j]]] <- est
### Recording estimates
for (k in 1:K){
Group.Est[[k]][j,] <- sparsify(grouplasso.result[[j]][(k-1)*p+(D-1)*(p) + (1:p)], Thresh2)
Sep.Est.Second[[k]][j,] <- sparsify(seplasso.result[[j]][(k-1)*p+(D-1)*(p) + (1:p)], Thresh2)
}
if (it>1){
#####
## STOPPING CRITERION for the two-stage estimation algorithm
#####
diff_magnitudes <- sum((c(grouplasso.result[[j]],seplasso.result[[j]])-c(grouplasso.result_before[[j]],seplasso.result_before[[j]]))^2)
## Printing out current difference in optimized values for consecutive iterations.
print(paste("Convergence:", diff_magnitudes))
if (diff_magnitudes < eps){
flag <- 1
#n.iter[run,j] <- it
}
}
### keeping track of estimates from previous iterations
grouplasso.result_before <- grouplasso.result
seplasso.result_before <- seplasso.result
}
#if (it == max.iter) n.iter[run,j] <- max.iter
# print(it)
}
Group.Est.Common <- list()
Group.Est.Common <- Group.Est
for(k in 1:K){
Group.Final[[k]] <- Group.Est[[k]] + Sep.Est.Second[[k]]
Final.Comm.Est[[run]][[k]] <- Group.Est[[k]]
Final.Ind.Est[[run]][[k]] <- Sep.Est.Second[[k]]
Group.Final[[k]] <- sparsify(Group.Final[[k]],Thresh2)
Final.Est[[run]][[k]] <- Group.Final[[k]]
}
#saveRDS(Final.Est,paste(namedir,"/",itr,"_","Final_Est.rds",sep=""))
#saveRDS(Final.Comm.Est,paste(namedir,"/",itr,"_","Common_Est.rds",sep=""))
#saveRDS(Final.Ind.Est,paste(namedir,"/",itr,"_","Ind_Est.rds",sep=""))
write.csv(Final.Est,paste(namedir,"K",K,"_Final_Est_",density[dd],"percent_",itr,".csv",sep=""))
write.csv(Final.Comm.Est,paste(namedir,"K",K,"Common_Est_",density[dd],"percent_",itr,".csv",sep=""))
write.csv(Final.Ind.Est,paste(namedir,"K",K,"Ind_Est_",density[dd],"percent_",itr,".csv",sep=""))
}
}
criter
setwd("C:/Users/CU_EE_LAB408/Dropbox/0MASTER/MATLAB_MASTER/JGranger_ncvx/experiment/Experiment_compare/skrip_code_2")
source('C:/Users/CU_EE_LAB408/Dropbox/0MASTER/MATLAB_MASTER/JGranger_ncvx/experiment/Experiment_compare/skrip_code_2/TWO_STAGE_ourformat_3.R', echo=TRUE)
source('C:/Users/CU_EE_LAB408/Dropbox/0MASTER/MATLAB_MASTER/JGranger_ncvx/experiment/Experiment_compare/skrip_code_2/TWO_STAGE_ourformat_3.R', echo=TRUE)
source('C:/Users/CU_EE_LAB408/Dropbox/0MASTER/MATLAB_MASTER/JGranger_ncvx/experiment/Experiment_compare/skrip_code_2/TWO_STAGE_ourformat_3.R', echo=TRUE)
Final.Est <- make.list(make.list(matrix(0,D*p,p),K),1)
Final.Comm.Est <- make.list(make.list(matrix(0,D*p,p),K),1)
Final.Ind.Est <-  make.list(make.list(matrix(0,D*p,p),K),1)
run=1
for(k in 1:K){
Group.Final[[k]] <- Group.Est[[k]] + Sep.Est.Second[[k]]
Final.Comm.Est[[run]][[k]] <- Group.Est[[k]]
Final.Ind.Est[[run]][[k]] <- Sep.Est.Second[[k]]
Group.Final[[k]] <- sparsify(Group.Final[[k]],Thresh2)
Final.Est[[run]][[k]] <- Group.Final[[k]]
}
write.csv(Final.Est,paste(namedir,"K",K,"_Final_Est_",density[dd],"percent_",itr,".csv",sep=""))
write.csv(Final.Comm.Est,paste(namedir,"K",K,"Common_Est_",density[dd],"percent_",itr,".csv",sep=""))
write.csv(Final.Ind.Est,paste(namedir,"K",K,"Ind_Est_",density[dd],"percent_",itr,".csv",sep=""))
itr
source('C:/Users/CU_EE_LAB408/Dropbox/0MASTER/MATLAB_MASTER/JGranger_ncvx/experiment/Experiment_compare/skrip_code_2/TWO_STAGE_ourformat_3.R', echo=TRUE)
